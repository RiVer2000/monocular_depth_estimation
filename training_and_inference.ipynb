{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce GTX 1660 Ti for training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA is not available. Exiting...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "print(f\"Using {device_name} for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import PIL\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "import segmentation_models_pytorch as smp\n",
    "# import albumentations as A  # Alternative for augmentations like `imutils`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "INIT_LR = 0.0001\n",
    "EPOCHS = 15\n",
    "TRAIN_PATH = \"../nyu_data/data/nyu2_train.csv\"\n",
    "TEST_PATH = \"../nyu_data/data/nyu2_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use torchvision.model.segmentation.fcn_resnet50\n",
    "model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "              ReLU-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "             ReLU-10           [-1, 64, 32, 32]               0\n",
      "       BasicBlock-11           [-1, 64, 32, 32]               0\n",
      "           Conv2d-12           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
      "             ReLU-14           [-1, 64, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 32, 32]             128\n",
      "             ReLU-17           [-1, 64, 32, 32]               0\n",
      "       BasicBlock-18           [-1, 64, 32, 32]               0\n",
      "           Conv2d-19           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
      "             ReLU-21           [-1, 64, 32, 32]               0\n",
      "           Conv2d-22           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 32, 32]             128\n",
      "             ReLU-24           [-1, 64, 32, 32]               0\n",
      "       BasicBlock-25           [-1, 64, 32, 32]               0\n",
      "           Conv2d-26          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 16, 16]             256\n",
      "             ReLU-28          [-1, 128, 16, 16]               0\n",
      "           Conv2d-29          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 16, 16]             256\n",
      "           Conv2d-31          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 16, 16]             256\n",
      "             ReLU-33          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-34          [-1, 128, 16, 16]               0\n",
      "           Conv2d-35          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
      "             ReLU-37          [-1, 128, 16, 16]               0\n",
      "           Conv2d-38          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 16, 16]             256\n",
      "             ReLU-40          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-41          [-1, 128, 16, 16]               0\n",
      "           Conv2d-42          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 16, 16]             256\n",
      "             ReLU-44          [-1, 128, 16, 16]               0\n",
      "           Conv2d-45          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 16, 16]             256\n",
      "             ReLU-47          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-48          [-1, 128, 16, 16]               0\n",
      "           Conv2d-49          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
      "             ReLU-51          [-1, 128, 16, 16]               0\n",
      "           Conv2d-52          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 16, 16]             256\n",
      "             ReLU-54          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-55          [-1, 128, 16, 16]               0\n",
      "           Conv2d-56            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-57            [-1, 256, 8, 8]             512\n",
      "             ReLU-58            [-1, 256, 8, 8]               0\n",
      "           Conv2d-59            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-60            [-1, 256, 8, 8]             512\n",
      "           Conv2d-61            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-62            [-1, 256, 8, 8]             512\n",
      "             ReLU-63            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-64            [-1, 256, 8, 8]               0\n",
      "           Conv2d-65            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
      "             ReLU-67            [-1, 256, 8, 8]               0\n",
      "           Conv2d-68            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-69            [-1, 256, 8, 8]             512\n",
      "             ReLU-70            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-71            [-1, 256, 8, 8]               0\n",
      "           Conv2d-72            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-73            [-1, 256, 8, 8]             512\n",
      "             ReLU-74            [-1, 256, 8, 8]               0\n",
      "           Conv2d-75            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-76            [-1, 256, 8, 8]             512\n",
      "             ReLU-77            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-78            [-1, 256, 8, 8]               0\n",
      "           Conv2d-79            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-80            [-1, 256, 8, 8]             512\n",
      "             ReLU-81            [-1, 256, 8, 8]               0\n",
      "           Conv2d-82            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 8, 8]             512\n",
      "             ReLU-84            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-85            [-1, 256, 8, 8]               0\n",
      "           Conv2d-86            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-87            [-1, 256, 8, 8]             512\n",
      "             ReLU-88            [-1, 256, 8, 8]               0\n",
      "           Conv2d-89            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-90            [-1, 256, 8, 8]             512\n",
      "             ReLU-91            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-92            [-1, 256, 8, 8]               0\n",
      "           Conv2d-93            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-94            [-1, 256, 8, 8]             512\n",
      "             ReLU-95            [-1, 256, 8, 8]               0\n",
      "           Conv2d-96            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-97            [-1, 256, 8, 8]             512\n",
      "             ReLU-98            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-99            [-1, 256, 8, 8]               0\n",
      "          Conv2d-100            [-1, 512, 4, 4]       1,179,648\n",
      "     BatchNorm2d-101            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-102            [-1, 512, 4, 4]               0\n",
      "          Conv2d-103            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-104            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-105            [-1, 512, 4, 4]         131,072\n",
      "     BatchNorm2d-106            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-107            [-1, 512, 4, 4]               0\n",
      "      BasicBlock-108            [-1, 512, 4, 4]               0\n",
      "          Conv2d-109            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-110            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-111            [-1, 512, 4, 4]               0\n",
      "          Conv2d-112            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-114            [-1, 512, 4, 4]               0\n",
      "      BasicBlock-115            [-1, 512, 4, 4]               0\n",
      "          Conv2d-116            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-117            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-118            [-1, 512, 4, 4]               0\n",
      "          Conv2d-119            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-120            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-121            [-1, 512, 4, 4]               0\n",
      "      BasicBlock-122            [-1, 512, 4, 4]               0\n",
      "   ResNetEncoder-123  [[-1, 3, 128, 128], [-1, 64, 64, 64], [-1, 64, 32, 32], [-1, 128, 16, 16], [-1, 256, 8, 8], [-1, 512, 4, 4]]               0\n",
      "        Identity-124            [-1, 512, 4, 4]               0\n",
      "        Identity-125            [-1, 768, 8, 8]               0\n",
      "       Attention-126            [-1, 768, 8, 8]               0\n",
      "          Conv2d-127            [-1, 256, 8, 8]       1,769,472\n",
      "     BatchNorm2d-128            [-1, 256, 8, 8]             512\n",
      "            ReLU-129            [-1, 256, 8, 8]               0\n",
      "          Conv2d-130            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-131            [-1, 256, 8, 8]             512\n",
      "            ReLU-132            [-1, 256, 8, 8]               0\n",
      "        Identity-133            [-1, 256, 8, 8]               0\n",
      "       Attention-134            [-1, 256, 8, 8]               0\n",
      "    DecoderBlock-135            [-1, 256, 8, 8]               0\n",
      "        Identity-136          [-1, 384, 16, 16]               0\n",
      "       Attention-137          [-1, 384, 16, 16]               0\n",
      "          Conv2d-138          [-1, 128, 16, 16]         442,368\n",
      "     BatchNorm2d-139          [-1, 128, 16, 16]             256\n",
      "            ReLU-140          [-1, 128, 16, 16]               0\n",
      "          Conv2d-141          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-142          [-1, 128, 16, 16]             256\n",
      "            ReLU-143          [-1, 128, 16, 16]               0\n",
      "        Identity-144          [-1, 128, 16, 16]               0\n",
      "       Attention-145          [-1, 128, 16, 16]               0\n",
      "    DecoderBlock-146          [-1, 128, 16, 16]               0\n",
      "        Identity-147          [-1, 192, 32, 32]               0\n",
      "       Attention-148          [-1, 192, 32, 32]               0\n",
      "          Conv2d-149           [-1, 64, 32, 32]         110,592\n",
      "     BatchNorm2d-150           [-1, 64, 32, 32]             128\n",
      "            ReLU-151           [-1, 64, 32, 32]               0\n",
      "          Conv2d-152           [-1, 64, 32, 32]          36,864\n",
      "     BatchNorm2d-153           [-1, 64, 32, 32]             128\n",
      "            ReLU-154           [-1, 64, 32, 32]               0\n",
      "        Identity-155           [-1, 64, 32, 32]               0\n",
      "       Attention-156           [-1, 64, 32, 32]               0\n",
      "    DecoderBlock-157           [-1, 64, 32, 32]               0\n",
      "        Identity-158          [-1, 128, 64, 64]               0\n",
      "       Attention-159          [-1, 128, 64, 64]               0\n",
      "          Conv2d-160           [-1, 32, 64, 64]          36,864\n",
      "     BatchNorm2d-161           [-1, 32, 64, 64]              64\n",
      "            ReLU-162           [-1, 32, 64, 64]               0\n",
      "          Conv2d-163           [-1, 32, 64, 64]           9,216\n",
      "     BatchNorm2d-164           [-1, 32, 64, 64]              64\n",
      "            ReLU-165           [-1, 32, 64, 64]               0\n",
      "        Identity-166           [-1, 32, 64, 64]               0\n",
      "       Attention-167           [-1, 32, 64, 64]               0\n",
      "    DecoderBlock-168           [-1, 32, 64, 64]               0\n",
      "          Conv2d-169         [-1, 16, 128, 128]           4,608\n",
      "     BatchNorm2d-170         [-1, 16, 128, 128]              32\n",
      "            ReLU-171         [-1, 16, 128, 128]               0\n",
      "          Conv2d-172         [-1, 16, 128, 128]           2,304\n",
      "     BatchNorm2d-173         [-1, 16, 128, 128]              32\n",
      "            ReLU-174         [-1, 16, 128, 128]               0\n",
      "        Identity-175         [-1, 16, 128, 128]               0\n",
      "       Attention-176         [-1, 16, 128, 128]               0\n",
      "    DecoderBlock-177         [-1, 16, 128, 128]               0\n",
      "     UnetDecoder-178         [-1, 16, 128, 128]               0\n",
      "          Conv2d-179          [-1, 1, 128, 128]             145\n",
      "        Identity-180          [-1, 1, 128, 128]               0\n",
      "        Identity-181          [-1, 1, 128, 128]               0\n",
      "      Activation-182          [-1, 1, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 24,436,369\n",
      "Trainable params: 24,436,369\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 82.12\n",
      "Params size (MB): 93.22\n",
      "Estimated Total Size (MB): 175.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, HEIGHT, WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read CSV file and load image-depth map pairs\n",
    "def read_csv(csv_file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns a list of tuples (image_path, depth_map_path).\n",
    "    \"\"\"\n",
    "    with open(csv_file_path, 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "        return [(f'./{row[0]}', f'./{row[1]}') for row in csv_reader if len(row) > 0]\n",
    "\n",
    "# Function to split the dataset into training and validation sets\n",
    "def train_val_split(train_paths, val_size):\n",
    "    \"\"\"\n",
    "    Splits the paths into training and validation datasets.\n",
    "    \"\"\"\n",
    "    random.shuffle(train_paths)  # Shuffle paths\n",
    "    len_train_paths = len(train_paths)\n",
    "    i = int(len_train_paths * (1.0 - val_size))\n",
    "    train = train_paths[:i]\n",
    "    val = train_paths[i:len(train_paths)]\n",
    "    return train, val\n",
    "\n",
    "# Function to load training paths and labels\n",
    "def load_train_paths(train_path):\n",
    "    \"\"\"\n",
    "    Loads the training paths and labels from the CSV file.\n",
    "    \"\"\"\n",
    "    train_paths = read_csv(train_path)\n",
    "    labels = {img_path: dm_path for img_path, dm_path in train_paths}  # Map image to depth map\n",
    "    x_paths = [img_path for img_path, _ in train_paths]  # Get list of image paths\n",
    "    x_train_paths, x_val_paths = train_val_split(x_paths, 0.3)  # Split into training and validation sets\n",
    "\n",
    "    partition = {\n",
    "        'train': x_train_paths,\n",
    "        'validation': x_val_paths\n",
    "    }\n",
    "    return partition, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from skimage.transform import resize\n",
    "from skimage import io\n",
    "\n",
    "# Normalize image\n",
    "def normalize_img(img):\n",
    "    \"\"\"\n",
    "    Normalizes an image to the range [0, 1].\n",
    "    \"\"\"\n",
    "    norm_img = (img - img.min()) / (img.max() - img.min())\n",
    "    return norm_img\n",
    "\n",
    "# Preprocess input image\n",
    "# def preprocess_image(img_path, horizontal_flip=False, height=128):\n",
    "#     \"\"\"\n",
    "#     Reads and preprocesses an RGB image.\n",
    "#     Args:\n",
    "#         img_path (str): Path to the image.\n",
    "#         horizontal_flip (bool): Whether to apply horizontal flip.\n",
    "#         height (int): Target height for resizing.\n",
    "#     Returns:\n",
    "#         torch.Tensor: Preprocessed image tensor.\n",
    "#     \"\"\"\n",
    "#     # Read and resize image\n",
    "#     image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "#     image = resize(image, (height, int(height * 4 / 3)), mode='reflect', preserve_range=True)\n",
    "#     image = image[:, 21:149].astype(np.float32)  # Crop and convert to float32\n",
    "#     image = normalize_img(image)  # Normalize to [0, 1]\n",
    "\n",
    "#     # Apply horizontal flip if needed\n",
    "#     if horizontal_flip:\n",
    "#         image = cv2.flip(image, 1)\n",
    "\n",
    "#     # Convert to PyTorch tensor (C, H, W)\n",
    "#     image = torch.tensor(image.transpose(2, 0, 1), dtype=torch.float32)\n",
    "\n",
    "#     # Debug statement to check if the image path is valid and the image is loaded\n",
    "#     print(f\"Image path: {img_path}\")\n",
    "#     return image\n",
    "def preprocess_image(image_path, res):\n",
    "    # Correct the path\n",
    "    base_path = \"/home/river2000/monocular_depth_estimation/nyu_data/data/\"\n",
    "    full_image_path = os.path.join(base_path, image_path)\n",
    "    \n",
    "    image = io.imread(full_image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image at path {full_image_path} could not be loaded.\")\n",
    "    height, width = res\n",
    "    image = resize(image, (height, int(height * 4 / 3)), mode='reflect', preserve_range=True)\n",
    "    return image\n",
    "\n",
    "# Preprocess depth map\n",
    "def preprocess_depth_map(depth_map_path, horizontal_flip=False, height=128):\n",
    "    \"\"\"\n",
    "    Reads and preprocesses a depth map.\n",
    "    Args:\n",
    "        depth_map_path (str): Path to the depth map.\n",
    "        horizontal_flip (bool): Whether to apply horizontal flip.\n",
    "        height (int): Target height for resizing.\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed depth map tensor.\n",
    "    \"\"\"\n",
    "    # Read and resize depth map\n",
    "    depth_map = cv2.imread(depth_map_path, cv2.IMREAD_GRAYSCALE)\n",
    "    depth_map = resize(depth_map, (height, int(height * 4 / 3)), mode='reflect', preserve_range=True)\n",
    "    depth_map = depth_map[:, 21:149].astype(np.float32)  # Crop and convert to float32\n",
    "    depth_map = normalize_img(depth_map)  # Normalize to [0, 1]\n",
    "\n",
    "    # Apply horizontal flip if needed\n",
    "    if horizontal_flip:\n",
    "        depth_map = cv2.flip(depth_map, 1)\n",
    "\n",
    "    # Add channel dimension and convert to PyTorch tensor\n",
    "    depth_map = torch.tensor(depth_map[np.newaxis, :, :], dtype=torch.float32)\n",
    "    # Debug statement to check if the depth map path is valid and the depth map is loaded\n",
    "    print(f\"Depth map path: {depth_map_path}\")\n",
    "    return depth_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the functions\n",
    "# img_path = \"../nyu_data/data/nyu2_train/basement_0001a_out/1.jpg\"\n",
    "# depth_map_path = \"../nyu_data/data/nyu2_train/basement_0001a_out/1.png\"\n",
    "# image = preprocess_image(img_path)\n",
    "# depth_map = preprocess_depth_map(depth_map_path)\n",
    "# print(f\"Image shape: {image.shape}, Depth map shape: {depth_map.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    def __init__(self, list_IDs, labels, dim=(128, 128), n_channels=3, batch_size=16, shuffle=True, pred=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            list_IDs (list): List of image file paths.\n",
    "            labels (dict): Dictionary mapping image paths to depth map paths.\n",
    "            dim (tuple): Dimensions of the input images (H, W).\n",
    "            n_channels (int): Number of input channels.\n",
    "            batch_size (int): Batch size.\n",
    "            shuffle (bool): Whether to shuffle the data at the end of each epoch.\n",
    "            pred (bool): If True, only generates inputs (no labels).\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.pred = pred\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of batches per epoch.\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates one batch of data.\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        if self.pred:\n",
    "            X = self.__data_generation(list_IDs_temp)\n",
    "            return X\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Updates indexes after each epoch.\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \"\"\"\n",
    "        Generates data containing batch_size samples.\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels))\n",
    "\n",
    "        if not self.pred:\n",
    "            y = np.empty((self.batch_size, self.dim[0], self.dim[1], 1))\n",
    "\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                res = random.choice([True, False])\n",
    "                X[i,] = preprocess_image(ID, res)\n",
    "                y[i,] = preprocess_depth_map(self.labels[ID], res)\n",
    "\n",
    "            return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "        else:\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                res = random.choice([True, False])\n",
    "                X[i,] = preprocess_image(ID, res)\n",
    "            return torch.tensor(X, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition, labels = load_train_paths(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35481 15207\n"
     ]
    }
   ],
   "source": [
    "print(len(partition['train']), len(partition['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels = labels, batch_size=16, dim=(128, 128), n_channels=3, shuffle=True, pred=False)\n",
    "validation_generator = DataGenerator(partition['validation'], labels = labels, batch_size=16, dim=(128, 128), n_channels=3, shuffle=True, pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# def poly_decay(epoch, max_epochs, base_lr, power=1.0):\n",
    "#     \"\"\"\n",
    "#     Implements polynomial learning rate decay.\n",
    "#     Args:\n",
    "#         epoch (int): Current epoch.\n",
    "#         max_epochs (int): Total number of epochs.\n",
    "#         base_lr (float): Initial learning rate.\n",
    "#         power (float): Decay power.\n",
    "#     Returns:\n",
    "#         float: Adjusted learning rate.\n",
    "#     \"\"\"\n",
    "#     return base_lr * (1 - (epoch / float(max_epochs))) ** power\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = Adam(model.parameters(), lr=INIT_LR, amsgrad=True)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "# max_epochs = EPOCHS\n",
    "# scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: poly_decay(epoch, max_epochs, INIT_LR))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def depth_loss(y_true, y_pred, w1=1.0, w2=3.0, w3=0.1):\n",
    "    \"\"\"\n",
    "    Custom depth loss combining SSIM, edge loss, and depth loss.\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Ground truth depth map.\n",
    "        y_pred (torch.Tensor): Predicted depth map.\n",
    "        w1, w2, w3 (float): Weights for SSIM, edge loss, and depth loss, respectively.\n",
    "    Returns:\n",
    "        torch.Tensor: Combined loss.\n",
    "    \"\"\"\n",
    "    # Depth loss (L1 loss)\n",
    "    l_depth = torch.mean(torch.abs(y_pred - y_true))\n",
    "\n",
    "    # Edge loss\n",
    "    dy_true, dx_true = torch.gradient(y_true, dim=(2, 3))\n",
    "    dy_pred, dx_pred = torch.gradient(y_pred, dim=(2, 3))\n",
    "    l_edges = torch.mean(torch.abs(dy_pred - dy_true) + torch.abs(dx_pred - dx_true))\n",
    "\n",
    "    # SSIM loss\n",
    "    ssim_loss = 1 - ssim(y_true, y_pred)\n",
    "    l_ssim = torch.clamp(ssim_loss * 0.5, 0, 1)\n",
    "\n",
    "    # Combined loss\n",
    "    return (w1 * l_ssim) + (w2 * l_edges) + (w3 * l_depth)\n",
    "\n",
    "def ssim(y_true, y_pred, max_val=1.0):\n",
    "    \"\"\"\n",
    "    Computes Structural Similarity Index (SSIM).\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Ground truth tensor.\n",
    "        y_pred (torch.Tensor): Predicted tensor.\n",
    "        max_val (float): Maximum value in the tensors.\n",
    "    Returns:\n",
    "        torch.Tensor: SSIM value.\n",
    "    \"\"\"\n",
    "    c1 = (0.01 * max_val) ** 2\n",
    "    c2 = (0.03 * max_val) ** 2\n",
    "\n",
    "    mu_true = F.avg_pool2d(y_true, kernel_size=3, stride=1, padding=1)\n",
    "    mu_pred = F.avg_pool2d(y_pred, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    sigma_true_sq = F.avg_pool2d(y_true * y_true, kernel_size=3, stride=1, padding=1) - mu_true ** 2\n",
    "    sigma_pred_sq = F.avg_pool2d(y_pred * y_pred, kernel_size=3, stride=1, padding=1) - mu_pred ** 2\n",
    "    sigma_true_pred = F.avg_pool2d(y_true * y_pred, kernel_size=3, stride=1, padding=1) - mu_true * mu_pred\n",
    "\n",
    "    ssim_map = ((2 * mu_true * mu_pred + c1) * (2 * sigma_true_pred + c2)) / \\\n",
    "               ((mu_true ** 2 + mu_pred ** 2 + c1) * (sigma_true_sq + sigma_pred_sq + c2))\n",
    "    return ssim_map.mean()\n",
    "\n",
    "def depth_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes soft accuracy for depth maps.\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Ground truth tensor.\n",
    "        y_pred (torch.Tensor): Predicted tensor.\n",
    "    Returns:\n",
    "        torch.Tensor: Soft accuracy.\n",
    "    \"\"\"\n",
    "    return torch.mean((torch.round(y_true) == torch.round(y_pred)).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9428634643554688\n",
      "Accuracy: 0.50103759765625\n"
     ]
    }
   ],
   "source": [
    "# Example tensors\n",
    "y_true = torch.rand((4, 1, 128, 128))  # Ground truth\n",
    "y_pred = torch.rand((4, 1, 128, 128))  # Predictions\n",
    "\n",
    "# Calculate loss and accuracy\n",
    "loss = depth_loss(y_true, y_pred)\n",
    "accuracy = depth_acc(y_true, y_pred)\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Accuracy:\", accuracy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, train_loader, val_loader, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # Training Phase\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward Pass\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = depth_loss(y_batch, y_pred)\n",
    "\n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                y_pred = model(X_val)\n",
    "                val_loss += depth_loss(y_val, y_pred).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/river2000/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/river2000/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/river2000/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_90435/2027826872.py\", line 46, in __getitem__\n    X, y = self.__data_generation(list_IDs_temp)\n  File \"/tmp/ipykernel_90435/2027826872.py\", line 70, in __data_generation\n    X[i,] = preprocess_image(ID, res)\n  File \"/tmp/ipykernel_90435/1418762535.py\", line 48, in preprocess_image\n    image = io.imread(full_image_path)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/skimage/io/_io.py\", line 60, in imread\n    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/skimage/io/manage_plugins.py\", line 217, in call_plugin\n    return func(*args, **kwargs)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/skimage/io/_plugins/imageio_plugin.py\", line 11, in imread\n    out = np.asarray(imageio_imread(*args, **kwargs))\n  File \"/home/river2000/.local/lib/python3.10/site-packages/imageio/v3.py\", line 53, in imread\n    with imopen(uri, \"r\", **plugin_kwargs) as img_file:\n  File \"/home/river2000/.local/lib/python3.10/site-packages/imageio/core/imopen.py\", line 113, in imopen\n    request = Request(uri, io_mode, format_hint=format_hint, extension=extension)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/imageio/core/request.py\", line 248, in __init__\n    self._parse_uri(uri)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/imageio/core/request.py\", line 408, in _parse_uri\n    raise FileNotFoundError(\"No such file: '%s'\" % fn)\nFileNotFoundError: No such file: '/home/river2000/monocular_depth_estimation/nyu_data/data/data/nyu2_train/living_room_0019_out/149.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(validation_generator, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, epochs, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Training Phase\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     10\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/river2000/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/river2000/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/river2000/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_90435/2027826872.py\", line 46, in __getitem__\n    X, y = self.__data_generation(list_IDs_temp)\n  File \"/tmp/ipykernel_90435/2027826872.py\", line 70, in __data_generation\n    X[i,] = preprocess_image(ID, res)\n  File \"/tmp/ipykernel_90435/1418762535.py\", line 48, in preprocess_image\n    image = io.imread(full_image_path)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/skimage/io/_io.py\", line 60, in imread\n    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/skimage/io/manage_plugins.py\", line 217, in call_plugin\n    return func(*args, **kwargs)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/skimage/io/_plugins/imageio_plugin.py\", line 11, in imread\n    out = np.asarray(imageio_imread(*args, **kwargs))\n  File \"/home/river2000/.local/lib/python3.10/site-packages/imageio/v3.py\", line 53, in imread\n    with imopen(uri, \"r\", **plugin_kwargs) as img_file:\n  File \"/home/river2000/.local/lib/python3.10/site-packages/imageio/core/imopen.py\", line 113, in imopen\n    request = Request(uri, io_mode, format_hint=format_hint, extension=extension)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/imageio/core/request.py\", line 248, in __init__\n    self._parse_uri(uri)\n  File \"/home/river2000/.local/lib/python3.10/site-packages/imageio/core/request.py\", line 408, in _parse_uri\n    raise FileNotFoundError(\"No such file: '%s'\" % fn)\nFileNotFoundError: No such file: '/home/river2000/monocular_depth_estimation/nyu_data/data/data/nyu2_train/living_room_0019_out/149.jpg'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(training_generator, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(validation_generator, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# Train the Model\n",
    "train_model(model, train_loader, val_loader, optimizer, EPOCHS, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = depth_loss(y_batch, y_pred)\n",
    "            accuracy = depth_acc(y_batch, y_pred)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss/len(test_loader):.4f}, Test Accuracy: {total_accuracy/len(test_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset and DataLoader\n",
    "test_paths = read_csv(TEST_PATH)\n",
    "test_labels = {i: j for i, j in test_paths}\n",
    "\n",
    "test_dataset = DataGenerator(test_paths, labels=test_labels, dim=(128, 128), n_channels=3, shuffle=False, pred=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# Evaluate the Model\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualize Predictions\n",
    "model.eval()\n",
    "for i in range(5):  # Show 5 random samples\n",
    "    X, y_true = test_dataset[i]\n",
    "    X = X.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    y_pred = model(X).squeeze().cpu().detach().numpy()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Predicted Depth Map\")\n",
    "    plt.imshow(y_pred, cmap=\"plasma\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Ground Truth Depth Map\")\n",
    "    plt.imshow(y_true.squeeze(), cmap=\"plasma\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(X.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
