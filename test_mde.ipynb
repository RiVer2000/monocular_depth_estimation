{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import PIL\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "import segmentation_models_pytorch as smp\n",
    "import piq\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != 'cuda':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# Constants\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "INIT_LR = 0.0001\n",
    "EPOCHS = 15\n",
    "TRAIN_PATH = \"./data/nyu2_train.csv\"\n",
    "TEST_PATH = \"./data/nyu2_test.csv\"\n",
    "\n",
    "# Load dataset\n",
    "def read_csv(csv_file_path):\n",
    "    with open(csv_file_path, 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "        return [('./' + row[0], './' + row[1]) for row in csv_reader if len(row) > 0]\n",
    "\n",
    "def train_val_split(train_paths, val_size):\n",
    "    random.shuffle(train_paths)\n",
    "    len_train_paths = len(train_paths)\n",
    "    i = int(len_train_paths * (1.0 - val_size))\n",
    "    train = train_paths[0:i]\n",
    "    val = train_paths[i:len(train_paths)]\n",
    "    return train, val\n",
    "\n",
    "def load_train_paths(train_path):\n",
    "    train_paths = read_csv(train_path)\n",
    "    labels = {img_path: dm_path for img_path, dm_path in train_paths}\n",
    "    x_paths = [img_path for img_path, dm in train_paths]\n",
    "    x_train_paths, x_val_paths = train_val_split(x_paths, 0.3)\n",
    "\n",
    "    partition = {\n",
    "        'train': x_train_paths,\n",
    "        'validation': x_val_paths\n",
    "    }\n",
    "    return partition, labels\n",
    "\n",
    "# Preprocessing\n",
    "def normalize_img(img):\n",
    "    norm_img = (img - img.min()) / (img.max() - img.min())\n",
    "    return norm_img\n",
    "\n",
    "def preprocess_image(img_path, horizontal_flip=False):\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.resize(image, (WIDTH, HEIGHT)).astype(\"float\")\n",
    "    image = normalize_img(image)\n",
    "\n",
    "    if horizontal_flip:\n",
    "        image = cv2.flip(image, 1)\n",
    "    return image\n",
    "\n",
    "def preprocess_depth_map(depth_map_path, horizontal_flip=False):\n",
    "    depth_map = cv2.imread(depth_map_path, cv2.IMREAD_GRAYSCALE)\n",
    "    depth_map = cv2.resize(depth_map, (WIDTH, HEIGHT)).astype(\"float\")\n",
    "    depth_map = normalize_img(depth_map)\n",
    "\n",
    "    if horizontal_flip:\n",
    "        depth_map = cv2.flip(depth_map, 1)\n",
    "\n",
    "    depth_map = np.reshape(depth_map, (depth_map.shape[0], depth_map.shape[1], 1))\n",
    "    return depth_map\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, list_IDs, labels, transform=None, pred=False):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.pred = pred\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ID = self.list_IDs[index]\n",
    "        image = preprocess_image(ID)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.pred:\n",
    "            return image\n",
    "        depth_map = preprocess_depth_map(self.labels[ID])\n",
    "        if self.transform:\n",
    "            depth_map = self.transform(depth_map)\n",
    "        return image, depth_map\n",
    "\n",
    "partition, labels = load_train_paths(TRAIN_PATH)\n",
    "print(len(partition['train']), len(partition['validation']))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "training_set = DepthDataset(partition['train'], labels, transform=transform)\n",
    "training_loader = DataLoader(training_set, batch_size=16, shuffle=True)\n",
    "\n",
    "validation_set = DepthDataset(partition['validation'], labels, transform=transform)\n",
    "validation_loader = DataLoader(validation_set, batch_size=16, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = fcn_resnet50(pretrained=True)\n",
    "model.classifier[4] = nn.Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def depth_loss(y_true, y_pred):\n",
    "    w1, w2, w3 = 1.0, 3.0, 0.1\n",
    "\n",
    "    l_depth = torch.mean(torch.abs(y_pred - y_true))\n",
    "\n",
    "    # Compute gradients using finite differences\n",
    "    dy_true = y_true[:, :, 1:, :] - y_true[:, :, :-1, :]\n",
    "    dx_true = y_true[:, :, :, 1:] - y_true[:, :, :, :-1]\n",
    "    dy_pred = y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :]\n",
    "    dx_pred = y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1]\n",
    "\n",
    "    # Pad the tensors to ensure they have the same dimensions\n",
    "    dy_true = F.pad(dy_true, (0, 0, 1, 0), mode='replicate')\n",
    "    dx_true = F.pad(dx_true, (1, 0, 0, 0), mode='replicate')\n",
    "    dy_pred = F.pad(dy_pred, (0, 0, 1, 0), mode='replicate')\n",
    "    dx_pred = F.pad(dx_pred, (1, 0, 0, 0), mode='replicate')\n",
    "\n",
    "    l_edges = torch.mean(torch.abs(dy_pred - dy_true) + torch.abs(dx_pred - dx_true))\n",
    "\n",
    "    # Normalize y_true and y_pred to the range [0, 1]\n",
    "    y_true_norm = (y_true - y_true.min()) / (y_true.max() - y_true.min())\n",
    "    y_pred_norm = (y_pred - y_pred.min()) / (y_pred.max() - y_pred.min())\n",
    "\n",
    "    l_ssim = torch.clip((1 - piq.ssim(y_true_norm, y_pred_norm, data_range=1.0)) * 0.5, 0, 1)\n",
    "\n",
    "    return (w1 * l_ssim) + (w2 * l_edges) + (w3 * l_depth)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=INIT_LR, amsgrad=True)\n",
    "\n",
    "# Training\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, depth_maps in training_loader:\n",
    "        images, depth_maps = images.to(device).float(), depth_maps.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = depth_loss(depth_maps, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "test_set = DepthDataset(partition['test'], labels, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for images, depth_maps in test_loader:\n",
    "        images, depth_maps = images.to(device), depth_maps.to(device)\n",
    "        outputs = model(images)['out']\n",
    "        loss = depth_loss(depth_maps, outputs)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss/len(test_loader)}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"./model1.pth\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
